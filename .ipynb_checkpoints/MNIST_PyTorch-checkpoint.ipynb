{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.ByteTensor'>\n",
      "<class 'torch.FloatTensor'>\n",
      "torch.Size([28, 60000, 28])\n"
     ]
    }
   ],
   "source": [
    "train_data = train.train_data\n",
    "print(type(train_data))\n",
    "train_data = train.transform(train_data.numpy())\n",
    "print(type(train_data))\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Numpy Shape: (60000, 28, 28)\n",
      " -> Tensor Shape: torch.Size([60000, 28, 28])\n",
      " -> Transformed Shape: torch.Size([28, 60000, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print(' -> Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' -> Tensor Shape:', train.train_data.size())\n",
    "print(' -> Transformed Shape:', train_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing Neural Network (Linear with Backpropogation)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Linear Layer\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 548)\n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "        \n",
    "        self.fc2 = nn.Linear(548, 252)\n",
    "        self.bc2 = nn.BatchNorm1d(252)\n",
    "        \n",
    "        self.fc3 = nn.Linear(252, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        \n",
    "        # Calling Initial Layers \n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        \n",
    "        # Defining Activation Function\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        # Calling Second Layers \n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        \n",
    "        # Second Layer Activation Function\n",
    "        \n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h, dim = 1)\n",
    "        return out\n",
    "\n",
    "# Creating a model.\n",
    "model = Model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=784, out_features=548, bias=True)\n",
      "  (bc1): BatchNorm1d(548, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc2): Linear(in_features=548, out_features=252, bias=True)\n",
      "  (bc2): BatchNorm1d(252, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc3): Linear(in_features=252, out_features=10, bias=True)\n",
      ")\n",
      "<torch.optim.adam.Adam object at 0x12c310be0>\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "print(model)\n",
    "\n",
    "# Adam Optimizer\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=784, out_features=548, bias=True)\n",
       "  (bc1): BatchNorm1d(548, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear(in_features=548, out_features=252, bias=True)\n",
       "  (bc2): BatchNorm1d(252, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear(in_features=252, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [57664/60000 (96%)]\tLoss: 0.175999\n",
      " Train Epoch: 1 [57664/60000 (96%)]\tLoss: 0.097834\n",
      " Train Epoch: 2 [57664/60000 (96%)]\tLoss: 0.136811\n",
      " Train Epoch: 3 [57664/60000 (96%)]\tLoss: 0.127650\n",
      " Train Epoch: 4 [57664/60000 (96%)]\tLoss: 0.073986\n",
      " Train Epoch: 5 [57664/60000 (96%)]\tLoss: 0.060315\n",
      " Train Epoch: 6 [64/60000 (0%)]\tLoss: 0.045477"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(15):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get Samples\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model(data) \n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(y_pred, target)\n",
    "        losses.append(loss.data[0])\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        if batch_idx % 100 == 1:\n",
    "            print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.data[0]), \n",
    "                end='')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n",
    "evaluate_y = Variable(test_loader.dataset.test_labels)\n",
    "\n",
    "\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "d = pred.eq(evaluate_y.data).cpu()\n",
    "accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs in Numbers for testdataset.\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = evaluate_x[131]\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((img).reshape(28,28), cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
